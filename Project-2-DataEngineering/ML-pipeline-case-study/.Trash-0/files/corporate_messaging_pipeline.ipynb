{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tokens.append(lemmatizer.lemmatize(tok).lower().strip())\n",
    "    return clean_tokens\n",
    "\n",
    "def score(model, X_test, y_test):\n",
    "    score = model.score(X_test, y_test)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLinkExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        features = np.recarray(shape=(len(posts),),\n",
    "                               dtype=[('text', object), ('links', list)])\n",
    "        for i, text in enumerate(posts):\n",
    "            \n",
    "            link_matches = re.findall(exp, text)\n",
    "            for link in link_matches:\n",
    "                text = text.replace(link, \"\")\n",
    "            \n",
    "            features['text'][i] = text\n",
    "            features['links'][i] = link_matches\n",
    "\n",
    "        return features\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class LinkCountExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, link_matches):\n",
    "        return [{'contains_link': len(link_matches) > 0}\n",
    "#                  'num_sentences': text.count('.')}\n",
    "                for text in link_matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tokenize, ngram_range=(1,1))\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "model = Pipeline([\n",
    "    ('textlink', TextLinkExtractor()),\n",
    "\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('vectorizer', vectorizer),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('links_pipeline', Pipeline([\n",
    "                ('selector', ItemSelector(key='links')),\n",
    "                ('stats', LinkCountExtractor()),\n",
    "                ('vect', DictVectorizer()),\n",
    "            ])),\n",
    "\n",
    "        ],\n",
    "\n",
    "#         transformer_weights={\n",
    "#             'subject': 0.8,\n",
    "#             'body_bow': 0.5,\n",
    "#             'body_stats': 1.0,\n",
    "#         },\n",
    "    )),\n",
    "\n",
    "    ('rf', rf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
    "df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
    "X = df.text.values\n",
    "y = df.category.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Counts:\n",
      "Information    1823\n",
      "Action          456\n",
      "Dialogue        124\n",
      "Name: category, dtype: int64\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 81   0  23]\n",
      " [  0  27   7]\n",
      " [  3   1 459]]\n",
      "Labels: ['Action' 'Dialogue' 'Information']\n",
      "Accuracy: 0.943427620632\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "acc = score(model, X_test, y_test)\n",
    "\n",
    "print(\"Value Counts:\")\n",
    "print(df.category.value_counts())\n",
    "print(\"\\nConfusion Matrix:\\n\", mat)\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: Action  Prediction: Information\n",
      "Can the U.S. help break the cycle of global poverty and disease?  Discuss with Tamara Evans at Think Science Now http://bit.ly/8YCX2m \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "What is the bond between duct tape and #healthcare? See what @vrulon has to say http://t.co/b76CPJcT \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "Gen Y: 65% seek financial education &amp  guidance says latest research. View our infographic for more details. http://t.co/Lq2539U7W1 \n",
      "\n",
      "Correct: Information  Prediction: Action\n",
      "For 80 years @ApolloTheater has rocked #Harlem with #legends - proud to be a sponsor. Here's to many more! #Apollo80 http://t.co/q1VABZ9NWa \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "#HPV causes most cases of #cervicalcancer. During #CervicalHealthMonth, learn the facts: http://t.co/mFIH4yJCws \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "MONDAY'S RIDDLE: when was #NestlÌ©`s first soluble cocoa beverage, #Nesquik developed? Take a guess! http://t.co/VO3a1p4bKU \n",
      "\n",
      "Correct: Dialogue  Prediction: Information\n",
      "@MzVotny @ButlerStefani We agree that access to water is a human right http://t.co/4QwamQTOfH \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "7 Ways to Dress to Impress w/out breaking the bank ÛÒ Advice via CitiÛªs Work Wardrobe Style Workshop http://t.co/AjeiVd5hAc @womenandco \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "Have you joined our #merckformothers community on FB? http://t.co/p2OeJCF1EE \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "Our Chairman calls for a ban on replacing #food production with #fuel production. Read @Guardian's live blog: http://t.co/8MuXB8Pa #nestle \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "#CitiVolunteer Deb Connor rebuilds the homes of typhoon survivors in the #Philippines. Read her story: http://t.co/3KYzMgFjxJ \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "Starting now: 'Health is wealth' panel at #wef2014. Live webcast http://t.co/ntJBTtfvlo #davos \n",
      "\n",
      "Correct: Dialogue  Prediction: Information\n",
      "Congrats @OutLeadership ! - A newly launched cross-industry collaboration to champion #LGBT #diversity. http://t.co/1WzYq7b183 @ToddSears \n",
      "\n",
      "Correct: Information  Prediction: Dialogue\n",
      "#Evian supports #DrinkH2O! You are what you drink &amp when you #DrinkH2O you drink up! @urH2O @evianwater #Water #danone http://t.co/yXfE4zvrTA \n",
      "\n",
      "Correct: Dialogue  Prediction: Information\n",
      "From the Merck family to yours, we wish you a safe and happy holiday season. \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "Discuss patient safety and health information technology with TSN Member Vera Rulon at http://bit.ly/aCL7QB \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "Oct. 13 is National Metastatic #Breastcancer #Awareness Day  join Pfizer to raise awareness of advanced BC, for info visit: mbcnetwork.org \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "Pfizer celebrated National Health Center Week at Southwest Community Health Center in CT. Learn more-http://www.healthcenterweek.org/ \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "#BarclaysJBS are sponsoring WetWheels charity @jerseyboatshow come and show your support http://t.co/yvir0lqutf #BarclaysDuck \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "RT @BarclaysStock: Have a look at the top ten most popular stocks in our Top Ten Trades from the last week http://bit.ly/cpdycq \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "Find out more about some of the highly skilled women we work with all over the world: http://t.co/4dIbs5HI8c #IWD2014 \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "Follow #ForbesHealth Summit Û÷13 w/Mikael Dolsten from #Pfizer #R&amp D. Connecting Science w/ People Who Will Benefit From It \n",
      "\n",
      "Correct: Information  Prediction: Action\n",
      "@cheeseburgerabs We have voluntarily recalled limited amounts of Nesquik chocolate powder sold only in the US: http://t.co/FvWc7mUr \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "5 Reasons To Buy Citigroup Right Now:  http://t.co/lzs2LPmOm6 #Citigroup #BRK \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "...Visit us @NAPFnews 5-7 March for more workforce insights. #NAPF #pension http://t.co/eR9REnMww6 http://t.co/HCbfiJBJkB \n",
      "\n",
      "Correct: Dialogue  Prediction: Information\n",
      "Congratulations to @POFerries Workplace Choir of the Year! What a magnificent performance - deserved champions! #TheChoir \n",
      "\n",
      "Correct: Information  Prediction: Action\n",
      "Could one test cancer cells against various chemotherapy agents to see which works best? http://t.co/yevlIyhG via @thinksciencenow \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "Julie Howard, Deputy Coordinator for Development, USAID speaking at CSV Forum. Watch live discussion now: http://bit.ly/kfNo23 #csv2011 \n",
      "\n",
      "Correct: Dialogue  Prediction: Information\n",
      "Congratulations to @P&amp O Workplace Choir of the Year! What a magnificent performance - deserved champions! #TheChoir \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "#P4C2013 panel ÛÏArt and Science of Multi-Stakeholder CollaborationÛ has begun!  Follow along to hear our CMO talk about #MerckforMothers! \n",
      "\n",
      "Correct: Dialogue  Prediction: Information\n",
      "Wishing all of our followers a happy and healthy new year! Be Well! \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "A retrospective study on the lifetime risk of developing COPD was recently released. Weigh in here http://t.co/gEF922m0 \n",
      "\n",
      "Correct: Action  Prediction: Information\n",
      "Citi Foundation commits $250k to support families devastated by Typhoon #Haiyan. Join us: http://t.co/taZvEnBExM @RedCross \n",
      "\n",
      "Correct: Dialogue  Prediction: Information\n",
      "@morpheusweb Hello, just checked with customer service, they will contact you very soon to see what happened! Have a good day \n",
      "\n"
     ]
    }
   ],
   "source": [
    "incorrect = y_pred != y_test\n",
    "for x, yt, yp in zip(X_test[incorrect], y_test[incorrect], y_pred[incorrect]):\n",
    "    print('Correct: {}  Prediction: {}'.format(yt, yp))\n",
    "    print(x, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
